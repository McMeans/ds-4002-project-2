{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24ac68e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9efd3f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching tweets from 2020-10-01 to 2025-10-01\n",
      "Total days to search: 1826\n",
      "Tweets per day per symbol: 25\n",
      "Total tweets expected: 45,650\n",
      "Expected cost: ~$6.85\n"
     ]
    }
   ],
   "source": [
    "# ---- CONFIGURATION ----\n",
    "# symbols = [\"AAPL\", \"MSFT\", \"NVDA\"]  # Stock tickers to search for\n",
    "symbols = [\"AAPL\"]\n",
    "max_tweets_per_day = 25           # Maximum tweets per day per symbol\n",
    "output_dir = \"../data/twitter\"      # Directory to save tweet data\n",
    "# ------------------------\n",
    "\n",
    "# TwitterAPI.io configuration\n",
    "TWITTERAPI_BASE_URL = \"https://api.twitterapi.io\"\n",
    "\n",
    "# Calculate date range - October 1, 2020 to October 1, 2025\n",
    "start_date = datetime.datetime(2020, 10, 1)\n",
    "end_date = datetime.datetime(2025, 10, 1)\n",
    "# start_date = datetime.datetime(2024, 9, 1)\n",
    "# end_date = datetime.datetime(2024, 10, 1)\n",
    "\n",
    "print(f\"Searching tweets from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total days to search: {(end_date - start_date).days}\")\n",
    "print(f\"Tweets per day per symbol: {max_tweets_per_day}\")\n",
    "print(f\"Total tweets expected: {len(symbols) * max_tweets_per_day * (end_date - start_date).days:,}\")\n",
    "print(f\"Expected cost: ~${(len(symbols) * max_tweets_per_day * (end_date - start_date).days * 0.15 / 1000):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4f31a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TwitterAPI.io client initialized successfully\n",
      "üí∞ You have $0.1 in free credits to start with\n",
      "üîë Using headers: ['X-API-Key', 'Content-Type']\n"
     ]
    }
   ],
   "source": [
    "# TwitterAPI.io API key setup - CORRECTED VERSION\n",
    "def setup_twitterapi_client():\n",
    "    \"\"\"Setup TwitterAPI.io client with authentication\"\"\"\n",
    "    api_key = os.getenv('TWITTERAPI_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"‚ùå TWITTERAPI_API_KEY not found in environment variables\")\n",
    "        print(\"üìù Please add your API key to .env file:\")\n",
    "        print(\"   TWITTERAPI_API_KEY=your_api_key_here\")\n",
    "        print(\"üîó Get your free API key at: https://twitterapi.io\")\n",
    "        return None\n",
    "    \n",
    "    # TwitterAPI.io uses X-API-Key header (capital X)\n",
    "    headers = {\n",
    "        'X-API-Key': api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    return headers\n",
    "\n",
    "# Initialize the API client\n",
    "api_headers = setup_twitterapi_client()\n",
    "if api_headers:\n",
    "    print(\"‚úÖ TwitterAPI.io client initialized successfully\")\n",
    "    print(\"üí∞ You have $0.1 in free credits to start with\")\n",
    "    print(f\"üîë Using headers: {list(api_headers.keys())}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize TwitterAPI.io client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35da03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets_for_symbol_daily(symbol, start_date, end_date, max_tweets_per_day=50):\n",
    "    \"\"\"\n",
    "    Search for tweets containing a specific stock ticker, collecting max_tweets_per_day for each day\n",
    "    Only collects English language tweets\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Stock ticker symbol (e.g., 'AAPL')\n",
    "        start_date (datetime): Start date for search\n",
    "        end_date (datetime): End date for search\n",
    "        max_tweets_per_day (int): Maximum number of tweets to retrieve per day\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tweet data dictionaries\n",
    "    \"\"\"\n",
    "    if not api_headers:\n",
    "        print(\"‚ùå TwitterAPI.io client not initialized\")\n",
    "        return []\n",
    "    \n",
    "    all_tweets = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    print(f\"üîç Searching for ${symbol} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"üìÖ Processing {(end_date - start_date).days} days...\")\n",
    "    \n",
    "    while current_date < end_date:\n",
    "        # Create date range for this specific day\n",
    "        day_start = current_date\n",
    "        day_end = current_date + datetime.timedelta(days=1)\n",
    "        \n",
    "        # Create search query for this specific day - ADD ENGLISH LANGUAGE FILTER\n",
    "        query = f\"${symbol} lang:en since:{day_start.strftime('%Y-%m-%d')} until:{day_end.strftime('%Y-%m-%d')}\"\n",
    "        \n",
    "        # TwitterAPI.io advanced search endpoint\n",
    "        url = f\"{TWITTERAPI_BASE_URL}/twitter/tweet/advanced_search\"\n",
    "        \n",
    "        tweets_for_day = []\n",
    "        cursor = ''\n",
    "        \n",
    "        try:\n",
    "            while len(tweets_for_day) < max_tweets_per_day and cursor is not None:\n",
    "                params = {\n",
    "                    'query': query,\n",
    "                    'queryType': 'Latest',\n",
    "                    'cursor': cursor\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, headers=api_headers, params=params)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    if 'tweets' in data and data['tweets']:\n",
    "                        for tweet in data['tweets']:\n",
    "                            if len(tweets_for_day) >= max_tweets_per_day:\n",
    "                                break\n",
    "                            \n",
    "                            # Double-check language filter (backup check)\n",
    "                            tweet_lang = tweet.get('lang', 'unknown')\n",
    "                            if tweet_lang != 'en':\n",
    "                                continue  # Skip non-English tweets\n",
    "                                \n",
    "                            tweet_data = {\n",
    "                                'symbol': symbol,\n",
    "                                'tweet_id': tweet['id'],\n",
    "                                'text': tweet['text'],\n",
    "                                'created_at': tweet['createdAt'],\n",
    "                                'user_id': tweet['author']['id'],\n",
    "                                'username': tweet['author']['userName'],\n",
    "                                'user_name': tweet['author']['name'],\n",
    "                                'retweet_count': tweet['retweetCount'],\n",
    "                                'like_count': tweet['likeCount'],\n",
    "                                'reply_count': tweet['replyCount'],\n",
    "                                'quote_count': tweet['quoteCount'],\n",
    "                                'view_count': tweet.get('viewCount', 0),\n",
    "                                'is_reply': tweet['isReply'],\n",
    "                                'conversation_id': tweet.get('conversationId'),\n",
    "                                'url': tweet['url'],\n",
    "                                'lang': tweet_lang,  # Store the language for verification\n",
    "                                'search_date': current_date.strftime('%Y-%m-%d')\n",
    "                            }\n",
    "                            tweets_for_day.append(tweet_data)\n",
    "                        \n",
    "                        # Check for next page\n",
    "                        if data['has_next_page'] and data['next_cursor']:\n",
    "                            cursor = data['next_cursor']\n",
    "                        else:\n",
    "                            cursor = None\n",
    "                    else:\n",
    "                        cursor = None\n",
    "                else:\n",
    "                    print(f\"‚ùå API Error for ${symbol} on {current_date.strftime('%Y-%m-%d')}: {response.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting between requests\n",
    "            \n",
    "            all_tweets.extend(tweets_for_day)\n",
    "            \n",
    "            # Progress update every 30 days\n",
    "            if (current_date - start_date).days % 30 == 0:\n",
    "                print(f\"üìä Progress: {current_date.strftime('%Y-%m-%d')} - {len(all_tweets)} tweets collected so far\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching tweets for ${symbol} on {current_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        \n",
    "        # Move to next day\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "        # Rate limiting between days\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(all_tweets)} English tweets for ${symbol} across {(end_date - start_date).days} days\")\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "132fee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tweets_for_all_symbols_daily(symbols, start_date, end_date, max_tweets_per_day):\n",
    "    \"\"\"\n",
    "    Collect tweets for all symbols with daily limits\n",
    "    \"\"\"\n",
    "    all_tweets = []\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, symbol in enumerate(symbols):\n",
    "        print(f\"\\n--- Processing {symbol} ({i+1}/{len(symbols)}) ---\")\n",
    "        \n",
    "        # Search for tweets with daily limits\n",
    "        tweets = search_tweets_for_symbol_daily(symbol, start_date, end_date, max_tweets_per_day)\n",
    "        all_tweets.extend(tweets)\n",
    "        \n",
    "        # Rate limiting between symbols\n",
    "        if i < len(symbols) - 1:\n",
    "            print(\"‚è≥ Waiting 2 seconds before next symbol...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0abb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_tweets(tweets_data, output_dir):\n",
    "    \"\"\"\n",
    "    Process tweet data and save to CSV files\n",
    "    \"\"\"\n",
    "    if not tweets_data:\n",
    "        print(\"No tweets to process\")\n",
    "        return\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(tweets_data)\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    \n",
    "    # Sort by symbol and date\n",
    "    df = df.sort_values(['symbol', 'created_at'])\n",
    "    \n",
    "    # Save individual files per symbol in ../data/[STOCK TICKER]/ format\n",
    "    for symbol in df['symbol'].unique():\n",
    "        symbol_df = df[df['symbol'] == symbol]\n",
    "        \n",
    "        # Create symbol directory in ../data/[STOCK TICKER]/ format\n",
    "        symbol_dir = f\"../data/{symbol}\"  # This matches your existing stock data structure\n",
    "        os.makedirs(symbol_dir, exist_ok=True)\n",
    "        \n",
    "        # Save to CSV with correct date range\n",
    "        filename = f\"{symbol}_tweets_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "        filepath = os.path.join(symbol_dir, filename)\n",
    "        symbol_df.to_csv(filepath, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Saved {len(symbol_df)} tweets for {symbol} to {filepath}\")\n",
    "    \n",
    "    # Save combined file in twitter directory\n",
    "    combined_filename = f\"all_tweets_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "    combined_filepath = os.path.join(output_dir, combined_filename)\n",
    "    df.to_csv(combined_filepath, index=False)\n",
    "    print(f\"‚úÖ Saved combined file with {len(df)} tweets to {combined_filepath}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0baf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_twitterapi_connection():\n",
    "#     \"\"\"\n",
    "#     Test TwitterAPI.io connection using the advanced_search endpoint\n",
    "#     \"\"\"\n",
    "#     print(\"üß™ Testing TwitterAPI.io Advanced Search Connection...\")\n",
    "#     print(\"=\" * 60)\n",
    "    \n",
    "#     # Test 1: Check if API key is loaded\n",
    "#     if not api_headers:\n",
    "#         print(\"‚ùå API headers not found. Please check your .env file\")\n",
    "#         return False\n",
    "    \n",
    "#     print(\"‚úÖ API headers loaded successfully\")\n",
    "#     print(f\"üîë Using headers: {list(api_headers.keys())}\")\n",
    "    \n",
    "#     # Test 2: Test API connection with minimal request\n",
    "#     try:\n",
    "#         # Use the correct advanced search endpoint\n",
    "#         url = f\"{TWITTERAPI_BASE_URL}/twitter/tweet/advanced_search\"\n",
    "        \n",
    "#         # Test parameters based on the documentation\n",
    "#         params = {\n",
    "#             'query': '$AAPL',  # Stock ticker with $ symbol\n",
    "#             'queryType': 'Latest',  # Latest tweets\n",
    "#             'cursor': ''  # Start from beginning\n",
    "#         }\n",
    "        \n",
    "#         print(f\"üîç Testing search for: '{params['query']}'\")\n",
    "#         print(f\"üì° Making API request to: {url}\")\n",
    "#         print(f\"üìã Parameters: {params}\")\n",
    "        \n",
    "#         response = requests.get(url, headers=api_headers, params=params)\n",
    "        \n",
    "#         print(f\"üìä Response status: {response.status_code}\")\n",
    "        \n",
    "#         if response.status_code == 200:\n",
    "#             data = response.json()\n",
    "#             print(\"‚úÖ API connection successful!\")\n",
    "            \n",
    "#             if 'tweets' in data and len(data['tweets']) > 0:\n",
    "#                 print(f\"üìà Found {len(data['tweets'])} tweets\")\n",
    "                \n",
    "#                 # Show first tweet as example\n",
    "#                 first_tweet = data['tweets'][0]\n",
    "#                 print(\"\\nüìù Sample tweet:\")\n",
    "#                 print(f\"   Text: {first_tweet['text'][:100]}...\")\n",
    "#                 print(f\"   User: @{first_tweet['author']['userName']}\")\n",
    "#                 print(f\"   Date: {first_tweet['createdAt']}\")\n",
    "#                 print(f\"   Retweets: {first_tweet['retweetCount']}\")\n",
    "#                 print(f\"   Likes: {first_tweet['likeCount']}\")\n",
    "#                 print(f\"   URL: {first_tweet['url']}\")\n",
    "                \n",
    "#                 # Show pagination info\n",
    "#                 print(f\"\\nüìÑ Pagination:\")\n",
    "#                 print(f\"   Has next page: {data['has_next_page']}\")\n",
    "#                 print(f\"   Next cursor: {data['next_cursor']}\")\n",
    "                \n",
    "#                 return True\n",
    "#             else:\n",
    "#                 print(\"‚ö†Ô∏è API connected but no tweets found\")\n",
    "#                 print(\"   This might be normal if there are no recent tweets for the query\")\n",
    "#                 return True\n",
    "#         else:\n",
    "#             print(f\"‚ùå API Error: {response.status_code}\")\n",
    "#             print(f\"   Response: {response.text}\")\n",
    "#             return False\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Connection test failed: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # Run the corrected test\n",
    "# test_results = test_twitterapi_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b51722f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Complete Tweet Collection Pipeline\n",
      "============================================================\n",
      "\n",
      "üì° Step 1: Collecting tweets for all symbols...\n",
      "\n",
      "--- Processing AAPL (1/1) ---\n",
      "üîç Searching for $AAPL from 2020-10-01 to 2025-10-01\n",
      "üìÖ Processing 1826 days...\n",
      "üìä Progress: 2020-10-01 - 25 tweets collected so far\n",
      "üìä Progress: 2020-10-31 - 775 tweets collected so far\n",
      "üìä Progress: 2020-11-30 - 1525 tweets collected so far\n",
      "üìä Progress: 2020-12-30 - 2275 tweets collected so far\n",
      "üìä Progress: 2021-01-29 - 3025 tweets collected so far\n",
      "üìä Progress: 2021-02-28 - 3758 tweets collected so far\n",
      "üìä Progress: 2021-03-30 - 4507 tweets collected so far\n",
      "üìä Progress: 2021-04-29 - 5257 tweets collected so far\n",
      "üìä Progress: 2021-05-29 - 5979 tweets collected so far\n",
      "üìä Progress: 2021-06-28 - 6729 tweets collected so far\n",
      "üìä Progress: 2021-07-28 - 7477 tweets collected so far\n",
      "üìä Progress: 2021-08-27 - 8227 tweets collected so far\n",
      "üìä Progress: 2021-09-26 - 8977 tweets collected so far\n",
      "üìä Progress: 2021-10-26 - 9727 tweets collected so far\n",
      "üìä Progress: 2021-11-25 - 10477 tweets collected so far\n",
      "üìä Progress: 2021-12-25 - 11227 tweets collected so far\n",
      "üìä Progress: 2022-01-24 - 11977 tweets collected so far\n",
      "üìä Progress: 2022-02-23 - 12727 tweets collected so far\n",
      "üìä Progress: 2022-03-25 - 13477 tweets collected so far\n",
      "üìä Progress: 2022-04-24 - 14227 tweets collected so far\n",
      "üìä Progress: 2022-05-24 - 14977 tweets collected so far\n",
      "üìä Progress: 2022-06-23 - 15727 tweets collected so far\n",
      "üìä Progress: 2022-07-23 - 16477 tweets collected so far\n",
      "üìä Progress: 2022-08-22 - 17202 tweets collected so far\n",
      "üìä Progress: 2022-09-21 - 17952 tweets collected so far\n",
      "üìä Progress: 2022-10-21 - 18640 tweets collected so far\n",
      "üìä Progress: 2022-11-20 - 19390 tweets collected so far\n",
      "üìä Progress: 2022-12-20 - 20099 tweets collected so far\n",
      "üìä Progress: 2023-01-19 - 20761 tweets collected so far\n",
      "üìä Progress: 2023-02-18 - 21466 tweets collected so far\n",
      "üìä Progress: 2023-03-20 - 21951 tweets collected so far\n",
      "üìä Progress: 2023-04-19 - 22521 tweets collected so far\n",
      "üìä Progress: 2023-05-19 - 23175 tweets collected so far\n",
      "üìä Progress: 2023-06-18 - 23818 tweets collected so far\n",
      "üìä Progress: 2023-07-18 - 24427 tweets collected so far\n",
      "üìä Progress: 2023-08-17 - 25166 tweets collected so far\n",
      "üìä Progress: 2023-09-16 - 25760 tweets collected so far\n",
      "üìä Progress: 2023-10-16 - 26400 tweets collected so far\n",
      "üìä Progress: 2023-11-15 - 26982 tweets collected so far\n",
      "üìä Progress: 2023-12-15 - 27420 tweets collected so far\n",
      "üìä Progress: 2024-01-14 - 27749 tweets collected so far\n",
      "üìä Progress: 2024-02-13 - 28184 tweets collected so far\n",
      "üìä Progress: 2024-03-14 - 28470 tweets collected so far\n",
      "üìä Progress: 2024-04-13 - 28703 tweets collected so far\n",
      "üìä Progress: 2024-05-13 - 29011 tweets collected so far\n",
      "üìä Progress: 2024-06-12 - 29192 tweets collected so far\n",
      "üìä Progress: 2024-07-12 - 29529 tweets collected so far\n",
      "üìä Progress: 2024-08-11 - 29763 tweets collected so far\n",
      "üìä Progress: 2024-09-10 - 30376 tweets collected so far\n",
      "üìä Progress: 2024-10-10 - 31059 tweets collected so far\n",
      "üìä Progress: 2024-11-09 - 31748 tweets collected so far\n",
      "üìä Progress: 2024-12-09 - 32347 tweets collected so far\n",
      "üìä Progress: 2025-01-08 - 32789 tweets collected so far\n",
      "üìä Progress: 2025-02-07 - 33390 tweets collected so far\n",
      "üìä Progress: 2025-03-09 - 34005 tweets collected so far\n",
      "üìä Progress: 2025-04-08 - 34744 tweets collected so far\n",
      "üìä Progress: 2025-05-08 - 35494 tweets collected so far\n",
      "üìä Progress: 2025-06-07 - 36230 tweets collected so far\n",
      "üìä Progress: 2025-07-07 - 36778 tweets collected so far\n",
      "üìä Progress: 2025-08-06 - 37329 tweets collected so far\n",
      "üìä Progress: 2025-09-05 - 37971 tweets collected so far\n",
      "‚úÖ Found 38553 English tweets for $AAPL across 1826 days\n",
      "‚úÖ Step 1 Complete: Collected 38553 total tweets\n",
      "\n",
      "üíæ Step 2: Processing and saving tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/cdb9zr597xq7kdzbqnsgf72r0000gn/T/ipykernel_3858/308251385.py:13: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['created_at'] = pd.to_datetime(df['created_at'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 38553 tweets for AAPL to ../data/AAPL/AAPL_tweets_2020-10-01_2025-10-01.csv\n",
      "‚úÖ Saved combined file with 38553 tweets to ../data/twitter/all_tweets_2020-10-01_2025-10-01.csv\n",
      "‚úÖ Step 2 Complete: Processed and saved 38553 tweets\n",
      "\n",
      "üìä Step 3: Summary Statistics\n",
      "========================================\n",
      "Total tweets collected: 38553\n",
      "Date range: 2020-10-01 21:58:03+00:00 to 2025-09-30 23:59:51+00:00\n",
      "Tweets per symbol:\n",
      "symbol\n",
      "AAPL    38553\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average engagement per tweet: 10.81\n",
      "Most engaging tweet: Market close: $NVDA: -16.91% | $AAPL: +3.21%\n",
      "\n",
      "Why is DeepSeek great for Apple?\n",
      "\n",
      "Here's a breakdown o...\n",
      "\n",
      "üéâ Tweet collection pipeline completed successfully!\n",
      "üí∞ Actual cost: ~$5.78\n"
     ]
    }
   ],
   "source": [
    "# Master execution cell - runs the complete tweet collection pipeline\n",
    "print(\"üöÄ Starting Complete Tweet Collection Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Collect tweets for all symbols (daily approach)\n",
    "print(\"\\nüì° Step 1: Collecting tweets for all symbols...\")\n",
    "all_tweets = collect_tweets_for_all_symbols_daily(symbols, start_date, end_date, max_tweets_per_day)\n",
    "print(f\"‚úÖ Step 1 Complete: Collected {len(all_tweets)} total tweets\")\n",
    "\n",
    "# Step 2: Process and save the tweets\n",
    "if all_tweets:\n",
    "    print(\"\\nüíæ Step 2: Processing and saving tweets...\")\n",
    "    tweet_df = process_and_save_tweets(all_tweets, output_dir)\n",
    "    print(f\"‚úÖ Step 2 Complete: Processed and saved {len(tweet_df)} tweets\")\n",
    "    \n",
    "    # Step 3: Display summary statistics\n",
    "    print(\"\\nüìä Step 3: Summary Statistics\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total tweets collected: {len(tweet_df)}\")\n",
    "    print(f\"Date range: {tweet_df['created_at'].min()} to {tweet_df['created_at'].max()}\")\n",
    "    print(f\"Tweets per symbol:\")\n",
    "    print(tweet_df['symbol'].value_counts())\n",
    "    \n",
    "    # Calculate total engagement\n",
    "    tweet_df['total_engagement'] = tweet_df['retweet_count'] + tweet_df['like_count'] + tweet_df['reply_count']\n",
    "    print(f\"\\nAverage engagement per tweet: {tweet_df['total_engagement'].mean():.2f}\")\n",
    "    print(f\"Most engaging tweet: {tweet_df.loc[tweet_df['total_engagement'].idxmax(), 'text'][:100]}...\")\n",
    "    \n",
    "    print(\"\\nüéâ Tweet collection pipeline completed successfully!\")\n",
    "    print(f\"üí∞ Actual cost: ~${(len(tweet_df) * 0.15 / 1000):.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No tweets were collected - check your configuration and API key\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
